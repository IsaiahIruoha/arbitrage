{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9a14c-5b92-4f91-98b9-0edecd032ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5333b90f-f5f2-4f87-83c2-62f1dcbb48f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c309b7-765b-4e6e-913f-93752b08d04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7840d-b480-4f10-929e-64ed3b30b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A minimal implementation of Monte Carlo tree search (MCTS) in Python 3\n",
    "Luke Harold Miles, July 2019, Public Domain Dedication\n",
    "See also https://en.wikipedia.org/wiki/Monte_Carlo_tree_search\n",
    "https://gist.github.com/qpwo/c538c6f73727e254fdc7fab81024f6e1\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Pulled from the web to be examined and modified\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "    \"Monte Carlo tree searcher. First rollout the tree then choose a move.\"\n",
    "\n",
    "    def __init__(self, exploration_weight=1):\n",
    "        self.Q = defaultdict(int)  # total reward of each node\n",
    "        self.N = defaultdict(int)  # total visit count for each node\n",
    "        self.children = dict()  # children of each node\n",
    "        self.exploration_weight = exploration_weight\n",
    "\n",
    "    def choose(self, node):\n",
    "        \"Choose the best successor of node. (Choose a move in the game)\"\n",
    "        if node.is_terminal():\n",
    "            raise RuntimeError(f\"choose called on terminal node {node}\")\n",
    "\n",
    "        if node not in self.children:\n",
    "            return node.find_random_child()\n",
    "\n",
    "        def score(n):\n",
    "            if self.N[n] == 0:\n",
    "                return float(\"-inf\")  # avoid unseen moves\n",
    "            return self.Q[n] / self.N[n]  # average reward\n",
    "\n",
    "        return max(self.children[node], key=score)\n",
    "\n",
    "    def do_rollout(self, node):\n",
    "        \"Make the tree one layer better. (Train for one iteration.)\"\n",
    "        path = self._select(node)\n",
    "        leaf = path[-1]\n",
    "        self._expand(leaf)\n",
    "        reward = self._simulate(leaf)\n",
    "        self._backpropagate(path, reward)\n",
    "\n",
    "    def _select(self, node):\n",
    "        \"Find an unexplored descendent of `node`\"\n",
    "        path = []\n",
    "        while True:\n",
    "            path.append(node)\n",
    "            if node not in self.children or not self.children[node]:\n",
    "                # node is either unexplored or terminal\n",
    "                return path\n",
    "            unexplored = self.children[node] - self.children.keys()\n",
    "            if unexplored:\n",
    "                n = unexplored.pop()\n",
    "                path.append(n)\n",
    "                return path\n",
    "            node = self._uct_select(node)  # descend a layer deeper\n",
    "\n",
    "    def _expand(self, node):\n",
    "        \"Update the `children` dict with the children of `node`\"\n",
    "        if node in self.children:\n",
    "            return  # already expanded\n",
    "        self.children[node] = node.find_children()\n",
    "\n",
    "    def _simulate(self, node):\n",
    "        \"Returns the reward for a random simulation (to completion) of `node`\"\n",
    "        invert_reward = True\n",
    "        while True:\n",
    "            if node.is_terminal():\n",
    "                reward = node.reward()\n",
    "                return 1 - reward if invert_reward else reward\n",
    "            node = node.find_random_child()\n",
    "            invert_reward = not invert_reward\n",
    "\n",
    "    def _backpropagate(self, path, reward):\n",
    "        \"Send the reward back up to the ancestors of the leaf\"\n",
    "        for node in reversed(path):\n",
    "            self.N[node] += 1\n",
    "            self.Q[node] += reward\n",
    "            reward = 1 - reward  # 1 for me is 0 for my enemy, and vice versa\n",
    "\n",
    "    def _uct_select(self, node):\n",
    "        \"Select a child of node, balancing exploration & exploitation\"\n",
    "\n",
    "        # All children of node should already be expanded:\n",
    "        assert all(n in self.children for n in self.children[node])\n",
    "\n",
    "        log_N_vertex = math.log(self.N[node])\n",
    "\n",
    "        def uct(n):\n",
    "            \"Upper confidence bound for trees\"\n",
    "            return self.Q[n] / self.N[n] + self.exploration_weight * math.sqrt(\n",
    "                log_N_vertex / self.N[n]\n",
    "            )\n",
    "\n",
    "        return max(self.children[node], key=uct)\n",
    "\n",
    "\n",
    "class Node(ABC):\n",
    "    \"\"\"\n",
    "    A representation of a single board state.\n",
    "    MCTS works by constructing a tree of these Nodes.\n",
    "    Could be e.g. a chess or checkers board state.\n",
    "    \"\"\"\n",
    "    def __init__(self, state, parent = None, parent_action = None):\n",
    "        #state will be a 2 by 2 array, the top two representing the opp, the\n",
    "        #bottom two representing the player\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.parent_action = parent_action\n",
    "        self.children = []\n",
    "        self._num_visits = 0\n",
    "        self._results = defaultdict(int)\n",
    "        self._results[1] = 0\n",
    "        self._results[-1] = 0\n",
    "        self._untried_actions = None\n",
    "        self._untried_actions = self.untried_actions()\n",
    "        return\n",
    "    \n",
    "    def untried_actions(self):\n",
    "        self._untried_actions = self.state.get_legal_actions()\n",
    "        return self._untried_actions\n",
    "    \n",
    "    def q(self):\n",
    "        wins = self._results[1]\n",
    "        loses = self._results[-1]\n",
    "        return wins - loses\n",
    "    \n",
    "    def n(self):\n",
    "        return self._num_visits\n",
    "    \n",
    "    #from present state, an action is chosen and the next state is generated based on that action\n",
    "    #the child node is created with the state of \"next state\" and appended to the list of children\n",
    "    #of the current node\n",
    "    def expand(self):\n",
    "        action = self._untried_actions.pop()\n",
    "        next_state = self.state.move(action)\n",
    "        child_node = Node(next_state, parent = self, parent_action = action)\n",
    "        \n",
    "        self.children.append(child_node)\n",
    "        return child_node\n",
    "    \n",
    "    def is_terminal_node(self):\n",
    "        return self.state.is_game_over()\n",
    "    \n",
    "    #for our application of MCTS, this step would be to run it through the neural network\n",
    "    #rather than do a full simulation\n",
    "    def rollout(self, result):\n",
    "        self._num_visits += 1\n",
    "        self._results[result] += 1\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(result)\n",
    "        return\n",
    "    \n",
    "    def backpropogate(self):\n",
    "        self._num_visits += 1.\n",
    "        self._results[result] += 1.\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(result)\n",
    "        return\n",
    "    \n",
    "    def is_fully_expanded(self):\n",
    "        return len(self._untried_actions) == 0\n",
    "    \n",
    "    def best_child(self, c_param = 0.1):\n",
    "        choices_weights = [(c.q() / c.n()) + c_param * \n",
    "                           np.sqrt((2 * np.log(self.n()) / c.n())) for c in self.children]\n",
    "        return self.children[np.argmax(choices_weights)]\n",
    "    \n",
    "    #randomly selects a next move (not necessary for our implementation)\n",
    "    def rollout_policy(self, possible_moves):\n",
    "        return possible_moves[np.random.randint(len(possible_moves))]\n",
    "    \n",
    "    def _tree_policy(self):\n",
    "        current_node = self\n",
    "        while not current_node.is_terminal_state():\n",
    "            if not current_node.is_fully_expanded():\n",
    "                return current_node.expand()\n",
    "            else:\n",
    "                current_node = current_node.best_child()\n",
    "        return current_node\n",
    "    \n",
    "    def best_action(self):\n",
    "        simulation_no = 100\n",
    "        \n",
    "        for i in range(simulation_no):\n",
    "            v = self._tree_policy()\n",
    "            #will have to change this for our implementation\n",
    "            reward = v.rollout()\n",
    "            v.backpropagate(reward)\n",
    "    \n",
    "    def get_legal_actions(self):\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    @abstractmethod\n",
    "    def find_children(self):\n",
    "        \"All possible successors of this board state\"\n",
    "        return set()\n",
    "\n",
    "    @abstractmethod\n",
    "    def find_random_child(self):\n",
    "        \"Random successor of this board state (for more efficient simulation)\"\n",
    "        return None\n",
    "\n",
    "    @abstractmethod\n",
    "    def is_terminal(self):\n",
    "        \"Returns True if the node has no children\"\n",
    "        return True\n",
    "\n",
    "    @abstractmethod\n",
    "    def reward(self):\n",
    "        \"Assumes `self` is terminal node. 1=win, 0=loss, .5=tie, etc\"\n",
    "        return 0\n",
    "\n",
    "    @abstractmethod\n",
    "    def __hash__(self):\n",
    "        \"Nodes must be hashable\"\n",
    "        return 123456789\n",
    "\n",
    "    @abstractmethod\n",
    "    def __eq__(node1, node2):\n",
    "        \"Nodes must be comparable\"\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a57f2-c994-44f1-a1e5-2f30e602ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    #states will be held in \n",
    "    def __init__(self):\n",
    "        state = defaultdict(list)\n",
    "        state['player'] = [1, 1]\n",
    "        state['opp'] = [1, 1]\n",
    "        return\n",
    "    \n",
    "    #mutator that takes the player_side (left or right) and the opp_side they want to act on\n",
    "    def player_move(self, plr_side, opp_side):\n",
    "        plr_side = -1\n",
    "        if plr_side == 'right':\n",
    "            plr_ind = 1\n",
    "        elif plr_side == 'left':\n",
    "            plr_ind = 0\n",
    "        else:\n",
    "            raise Exception(\"plr_side and opp_side arguments can only accept 'right' or 'left as input'\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf44c8-2e3b-4c5c-8503-3ee8dd8de764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Play_Chop:\n",
    "    def __init__(self):\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d3851-2ac8-4fec-860c-7ecbbe7e2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game_Chop:\n",
    "    #Generate the initial game state.\n",
    "    def __init__(self):\n",
    "        #TODO\n",
    "        \n",
    "    \n",
    "    #return current players legal moves from a given state \n",
    "    #(similar if not equivalent to findChildren)\n",
    "    def legalPlays(state):\n",
    "        #TODO\n",
    "        return plays\n",
    "    \n",
    "    #Advance the given state and return it\n",
    "    def nextState(state, move):\n",
    "        #TODO\n",
    "        return newState\n",
    "    \n",
    "    #return the reward from the game, i.e. reward, 1 if win, 0 if loss (can't tie in chopsticks)\n",
    "    def reward(state):\n",
    "        #TODO\n",
    "        return winner\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f694d3f-9ab9-43a2-9cbc-eec094b707c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
